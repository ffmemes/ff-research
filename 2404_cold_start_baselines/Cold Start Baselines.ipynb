{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Start Baselines\n",
    "\n",
    "Algorithms\n",
    "\n",
    "1. `best_memes_from_each_source`. Current production (with small diffs)\n",
    "2. `most_liked`. Same as prod but with randomization of top 100\n",
    "3. `best_memes_from_each_cluster`. Is based on custom clusters rather than sources\n",
    "\n",
    "Also, alternative meme stats were tried. The difference is that the alternative version only counts reactions from users that have less than 200 reactions.\n",
    "\n",
    "**Resolution**\n",
    "\n",
    "* Most likes on alternative stats go to AB test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "coldstart_df = pl.read_parquet('coldstart_dataset.pq')\n",
    "meme_features_daily_df = pl.read_parquet('meme_features_daily.pq')\n",
    "\n",
    "# v2 is calculated on users with less than 200 responses\n",
    "meme_features_daily_v2_df = pl.read_parquet('meme_features_daily_v2.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>hist_size</th><th>dtm</th><th>date_dtm</th><th>hist_memes</th><th>hist_reactions</th><th>target_memes</th><th>target_reactions</th></tr><tr><td>str</td><td>i64</td><td>datetime[μs]</td><td>datetime[μs]</td><td>list[str]</td><td>list[i64]</td><td>list[str]</td><td>list[i64]</td></tr></thead><tbody><tr><td>&quot;486,191,407&quot;</td><td>20</td><td>2024-04-03 19:20:00</td><td>2024-04-03 00:00:00</td><td>[&quot;1,237,876&quot;, &quot;2,829,942&quot;, … &quot;3,755,263&quot;]</td><td>[2, 2, … 1]</td><td>[&quot;1,197,484&quot;, &quot;3,546,640&quot;, … &quot;893&quot;]</td><td>[2, 2, … 2]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 8)\n",
       "┌────────────┬───────────┬────────────┬────────────┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id    ┆ hist_size ┆ dtm        ┆ date_dtm   ┆ hist_meme ┆ hist_reac ┆ target_me ┆ target_re │\n",
       "│ ---        ┆ ---       ┆ ---        ┆ ---        ┆ s         ┆ tions     ┆ mes       ┆ actions   │\n",
       "│ str        ┆ i64       ┆ datetime[μ ┆ datetime[μ ┆ ---       ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆ s]         ┆ s]         ┆ list[str] ┆ list[i64] ┆ list[str] ┆ list[i64] │\n",
       "╞════════════╪═══════════╪════════════╪════════════╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 486,191,40 ┆ 20        ┆ 2024-04-03 ┆ 2024-04-03 ┆ [\"1,237,8 ┆ [2, 2, …  ┆ [\"1,197,4 ┆ [2, 2, …  │\n",
       "│ 7          ┆           ┆ 19:20:00   ┆ 00:00:00   ┆ 76\", \"2,8 ┆ 1]        ┆ 84\", \"3,5 ┆ 2]        │\n",
       "│            ┆           ┆            ┆            ┆ 29,942\",  ┆           ┆ 46,640\",  ┆           │\n",
       "│            ┆           ┆            ┆            ┆ … \"3,…    ┆           ┆ … \"89…    ┆           │\n",
       "└────────────┴───────────┴────────────┴────────────┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coldstart_df.slice(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size = 3288 users, test size = 823 users\n"
     ]
    }
   ],
   "source": [
    "user_ids = coldstart_df.get_column('user_id').unique().to_list()\n",
    "train_user_ids, test_user_ids = train_test_split(user_ids, test_size=0.2, random_state=42)\n",
    "print(f'train size = {len(train_user_ids)} users, test size = {len(test_user_ids)} users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7376"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = (\n",
    "    coldstart_df\n",
    "    .filter(pl.col('user_id').is_in(train_user_ids))\n",
    "    .filter(pl.col('hist_size') <= 30)\n",
    ")\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4831"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_2w_df = (\n",
    "    coldstart_df\n",
    "    .filter(pl.col('date_dtm') >= datetime(2024, 4, 1))\n",
    "    .filter(pl.col('hist_size') <= 30)\n",
    "    .filter(pl.col('target_memes').list.len() < 100)\n",
    ")\n",
    "len(last_2w_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>meme_id</th><th>n_memes_sent</th><th>n_likes</th><th>n_dislikes</th><th>age_days</th><th>date_dtm</th><th>meme_source_id</th></tr><tr><td>str</td><td>u32</td><td>i64</td><td>i64</td><td>i64</td><td>datetime[μs]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;12,528&quot;</td><td>0</td><td>0</td><td>0</td><td>11</td><td>2024-03-01 00:00:00</td><td>46</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 7)\n",
       "┌─────────┬──────────────┬─────────┬────────────┬──────────┬─────────────────────┬────────────────┐\n",
       "│ meme_id ┆ n_memes_sent ┆ n_likes ┆ n_dislikes ┆ age_days ┆ date_dtm            ┆ meme_source_id │\n",
       "│ ---     ┆ ---          ┆ ---     ┆ ---        ┆ ---      ┆ ---                 ┆ ---            │\n",
       "│ str     ┆ u32          ┆ i64     ┆ i64        ┆ i64      ┆ datetime[μs]        ┆ i64            │\n",
       "╞═════════╪══════════════╪═════════╪════════════╪══════════╪═════════════════════╪════════════════╡\n",
       "│ 12,528  ┆ 0            ┆ 0       ┆ 0          ┆ 11       ┆ 2024-03-01 00:00:00 ┆ 46             │\n",
       "└─────────┴──────────────┴─────────┴────────────┴──────────┴─────────────────────┴────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meme_features_daily_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to production. Simplifications:\n",
    "# Memes without stats were omitted\n",
    "# Top impression feature is omitted (gives 1.0 vs 0.8 for top 1 meme from a source by its telegram impressions)\n",
    "# Impressions without reactions are omitted\n",
    "\n",
    "score = pl.when(pl.col('age_days') < 14).then(1.0).otherwise(0.8) * pl.col('n_likes') / (pl.col('n_likes') + pl.col('n_dislikes'))\n",
    "\n",
    "\n",
    "def best_meme_from_each_source(meme_features_daily_df, date_dtm):\n",
    "    return (\n",
    "        meme_features_daily_df\n",
    "        .filter(pl.col('date_dtm') == date_dtm)\n",
    "        .filter(pl.col('n_likes') + pl.col('n_dislikes') > 0)\n",
    "        .with_columns(score.alias('score'))\n",
    "        .sort('score', descending=True)\n",
    "        .group_by('meme_source_id')\n",
    "        .agg(pl.all().first())\n",
    "        .get_column('meme_id')\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "best_meme_from_each_source_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    best_meme_from_each_source_cache[date_dtm] = best_meme_from_each_source(meme_features_daily_df, date_dtm)\n",
    "\n",
    "best_meme_from_each_source_v2_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    best_meme_from_each_source_v2_cache[date_dtm] = best_meme_from_each_source(meme_features_daily_v2_df, date_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most liked\n",
    "\n",
    "score = pl.when(pl.col('age_days') < 14).then(1.0).otherwise(0.8) * pl.col('n_likes') / (pl.col('n_likes') + pl.col('n_dislikes'))\n",
    "\n",
    "\n",
    "def most_liked(meme_features_daily_df, date_dtm):\n",
    "    return (\n",
    "        meme_features_daily_df\n",
    "        .filter(pl.col('date_dtm') == date_dtm)\n",
    "        .filter(pl.col('n_memes_sent') > 10)\n",
    "        .with_columns(score.alias('score'))\n",
    "        .sort('score', descending=True)\n",
    "        .head(100)\n",
    "        .select(pl.col('meme_id').shuffle(int(date_dtm.timestamp())))\n",
    "        .get_column('meme_id')\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "most_liked_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    most_liked_cache[date_dtm] = most_liked(meme_features_daily_df, date_dtm)\n",
    "\n",
    "most_liked_v2_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    most_liked_v2_cache[date_dtm] = most_liked(meme_features_daily_v2_df, date_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to production. Simplifications:\n",
    "# Memes without stats were omitted\n",
    "# Top impression feature is omitted (gives 1.0 vs 0.8 for top 1 meme from a source by its telegram impressions)\n",
    "# Impressions without reactions are omitted\n",
    "\n",
    "score = pl.when(pl.col('age_days') < 14).then(1.0).otherwise(0.8) * pl.col('n_likes') / (pl.col('n_likes') + pl.col('n_dislikes'))\n",
    "\n",
    "def random_recs(user_id, meme_features_daily_df, date_dtm):\n",
    "    hash = int(hashlib.sha256(user_id.encode('utf-8')).hexdigest(), 16) % 10**8\n",
    "    return (\n",
    "        meme_features_daily_df\n",
    "        .filter(pl.col('date_dtm') == date_dtm)\n",
    "        .sample(1000, seed=hash)\n",
    "        .get_column('meme_id')\n",
    "        .to_list()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_seen(recs, hist_memes):\n",
    "    \"\"\"Filters memes that were seen\"\"\"\n",
    "    return [meme_id for meme_id in recs if meme_id not in hist_memes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate(recs, target_memes, target_reactions):\n",
    "    \"\"\"Matches recs with future seen memes from target list\n",
    "    Calculates likes and dislikes\"\"\"\n",
    "    likes = 0\n",
    "    dislikes = 0\n",
    "    for meme_id in recs:\n",
    "        if meme_id not in target_memes:\n",
    "            continue\n",
    "        idx = target_memes.index(meme_id)\n",
    "        reaction = target_reactions[idx]\n",
    "        if reaction == 1:\n",
    "            likes += 1\n",
    "            continue\n",
    "        if reaction == 2:\n",
    "            dislikes += 1\n",
    "            continue\n",
    "\n",
    "    if (likes + dislikes) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    lr = likes / (likes + dislikes)\n",
    "\n",
    "    return likes, dislikes, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 1829, Like Rate = 0.474 +-0.016\n"
     ]
    }
   ],
   "source": [
    "# Testing best_meme_from_each_source\n",
    "# Taking tops from the cache\n",
    "# Filtering seen\n",
    "# Taking top 100 (need to choose thresholds, for this alg top-10 performed similarly)\n",
    "\n",
    "rows = []\n",
    "for row in train_df.iter_rows(named=True):\n",
    "    recs = best_meme_from_each_source_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 710, Like Rate = 0.489 +-0.026\n"
     ]
    }
   ],
   "source": [
    "# Testing random recs\n",
    "# I think it has no sense due to biased dataset (no actually random memes in production)\n",
    "\n",
    "rows = []\n",
    "for row in train_df.iter_rows(named=True):\n",
    "    recs = random_recs(row['user_id'], meme_features_daily_df, row['date_dtm'])\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 6812, Like Rate = 0.613 +-0.009\n"
     ]
    }
   ],
   "source": [
    "# Testing most liked\n",
    "\n",
    "rows = []\n",
    "for row in train_df.iter_rows(named=True):\n",
    "    recs = most_liked_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing best meme from each cluster algorithm\n",
    "\n",
    "Clusters achieved using ALS + KMeans\n",
    "\n",
    "Num clusters = 10\n",
    "\n",
    "Validation details\n",
    "\n",
    "* Train = 1.03.24 - 1.04.24\n",
    "* Test = 1.04.24 - 13.04.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_clusters_df = pl.read_parquet('meme_clusters.pq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster based\n",
    "\n",
    "score = pl.when(pl.col('age_days') < 14).then(1.0).otherwise(0.8) * pl.col('n_likes') / (pl.col('n_likes') + pl.col('n_dislikes'))\n",
    "\n",
    "def best_memes_from_each_cluster(meme_features_daily_df, date_dtm):\n",
    "    return (\n",
    "        meme_features_daily_df\n",
    "        .filter(pl.col('date_dtm') == date_dtm)\n",
    "        .filter(pl.col('n_memes_sent') > 10)\n",
    "        .join(meme_clusters_df, on='meme_id')\n",
    "        .with_columns(score.alias('score'))\n",
    "        .sort('score', descending=True)\n",
    "        .group_by('cluster_id')\n",
    "        .agg(pl.col('meme_id').head(10))\n",
    "        .explode(pl.col('meme_id'))\n",
    "        .select(pl.col('meme_id').shuffle(int(date_dtm.timestamp())))\n",
    "        .get_column('meme_id')\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "best_memes_from_each_cluster_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    best_memes_from_each_cluster_cache[date_dtm] = best_memes_from_each_cluster(meme_features_daily_df, date_dtm)\n",
    "\n",
    "best_memes_from_each_cluster_v2_cache = {}\n",
    "for date_dtm in train_df.select('date_dtm').unique().get_column('date_dtm').to_list():\n",
    "    best_memes_from_each_cluster_v2_cache[date_dtm] = best_memes_from_each_cluster(meme_features_daily_v2_df, date_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 512, Like Rate = 0.537 +-0.032, Like Rate Micro = 0.536\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in last_2w_df.iter_rows(named=True):\n",
    "    recs = best_meme_from_each_source_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "lr_micro = results_df['lr'].mean()\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}, Like Rate Micro = {lr_micro:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 5215, Like Rate = 0.591 +-0.010, Like Rate Micro = 0.532\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in last_2w_df.iter_rows(named=True):\n",
    "    recs = best_memes_from_each_cluster_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "lr_micro = results_df['lr'].mean()\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}, Like Rate Micro = {lr_micro:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 5030, Like Rate = 0.573 +-0.010, Like Rate Micro = 0.526\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in last_2w_df.iter_rows(named=True):\n",
    "    recs = best_memes_from_each_cluster_v2_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "lr_micro = results_df['lr'].mean()\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}, Like Rate Micro = {lr_micro:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 1801, Like Rate = 0.567 +-0.017, Like Rate Micro = 0.504\n"
     ]
    }
   ],
   "source": [
    "# Testing most liked\n",
    "\n",
    "rows = []\n",
    "for row in last_2w_df.iter_rows(named=True):\n",
    "    recs = most_liked_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "lr_micro = results_df['lr'].mean()\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}, Like Rate Micro = {lr_micro:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likes - 5150, Like Rate = 0.578 +-0.010, Like Rate Micro = 0.531\n"
     ]
    }
   ],
   "source": [
    "# Testing most liked v2\n",
    "\n",
    "rows = []\n",
    "for row in last_2w_df.iter_rows(named=True):\n",
    "    recs = most_liked_v2_cache[row['date_dtm']]\n",
    "    recs = filter_seen(recs, row['hist_memes'])\n",
    "    recs = recs[:100]\n",
    "    likes, dislikes, lr = estimate(recs, row['target_memes'], row['target_reactions'])\n",
    "    rows.append({\n",
    "        'user_id': row['user_id'],\n",
    "        'hist_size': row['hist_size'],\n",
    "        'date_dtm': row['date_dtm'],\n",
    "        'likes': likes,\n",
    "        'dislikes': dislikes,\n",
    "        'lr': lr,\n",
    "    })\n",
    "results_df = pl.DataFrame(rows)\n",
    "\n",
    "likes = results_df['likes'].sum()\n",
    "n = results_df['likes'].sum() + results_df['dislikes'].sum()\n",
    "lr = results_df['likes'].sum() / (results_df['likes'].sum() + results_df['dislikes'].sum())\n",
    "lr_micro = results_df['lr'].mean()\n",
    "std = np.sqrt(lr * (1 - lr) / n)\n",
    "print(f'Likes - {likes}, Like Rate = {lr:.3f} +-{std * 1.98:.3f}, Like Rate Micro = {lr_micro:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most liked on meme_stats_v2 are going for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('meme_ids.json', 'w') as f:\n",
    "    ids = most_liked_v2_cache[datetime(2024, 4, 13)]\n",
    "    ids = [int(id.replace(',', '')) for id in ids]\n",
    "    json.dump(ids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to generate query string for the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/meme 3537906 961188 3486879 3940783 2829942 861412 1213515 1964789 3940785 3517077 10222 3772372 3902342 3516170 3644890 3316350 2895478 1103300 3546640 3823857 3600766 2279191 2086964 3454946 3564685 2694493 2886632 1623255 1994543 3513207 3574336 2350587 881987 3652728 3940729 3477742 3644716 1197491 3635346 3697354 3604881 2377586 3467378 3140410 1245424 3551923 3719351 3724910 1267028 3569939 229958 2740166 3746267 3495787 3604880 3644177 3533161 3702327 3520485 3539067 3875767 2625535 2279188 693384 3859587 1169378 2352815 3569301 2713467 2392622 3775191 10152 3855063 2005796 2050874 2249024 3471463 3592903 1988648 3745276 3925003 2501564 1825631 2953444 3520907 1202825 2821161 3551186 476907 1068395 3587199 1190719 2060890 3565438 3661643 3570595 2091903 3832346 321242 3574793'"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/meme ' + ' '.join([s.replace(',', '') for s in best_memes_from_each_cluster_cache[datetime(2024, 4, 10)][:100]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
